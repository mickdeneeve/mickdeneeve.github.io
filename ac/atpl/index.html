<html>
<head>
<title>Hypernymy recognition in a neural distributional model using information gain and
  update semantics</title>
</head>
<body>
<h1><center>Hypernymy recognition in a neural distributional model using information gain and update semantics<center></h1>
<h3><center>Advanced Topics in the Philosophy of Language<br>
Lecturer: Arianna Betti</center></h3>
<h3><center>Mick de Neeve &ltmick@live.nl&gt<br>Department of Philosophy<br>University of Amsterdam<br><br>December 23, 2022</center></h3>
<b>Abstract </b> While similarity relations can be easily identified in distributional models, hypernymy is harder because the relationship is asymmetric. Information gain has been proposed as a measure, but it is not straightforward how to use this in neural models because word co-occurences are implicit. This paper presents a method to nonetheless measure it in a (neural) Word2Vec model, but it has an additional aim: to show update semantics is a viable candidate for the underlying notion of meaning in distributional models. To this end, an epistemic dynamic semantics is implemented using the neural network, with updates between hypernymic information states. The result confirms information gain experiments to a signicant degree, and outperforms it on a small subset of transitivity triples.
<ul>
  <li><a href="mickdeneeve.atpl.pdf" target="_blank">Full text (pdf)</a></li>
  <br>
  <li><a href="src/">Python source code</a></li>
  <li><a href="lex/">Datasets</a></li>
  <li><a href="exp/">Results</a></li>
  <br>
  <li><a href="https://github.com/RaRe-Technologies/gensim" target="_blank">Model (Gensim/Word2Vec)</a></li>
  <li><a href="https://www.litika.com/torrents/enwiki-20210920-pages-articles.xml.bz2.torrent" target="_blank">Corpus (Wikipedia torrent)</a></li>
</ul> 
</body>
</html>
